<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>lijun note</title>
    <link href="https://pure419.github.io/feed.xml" rel="self" />
    <link href="https://pure419.github.io" />
    <updated>2024-06-03T18:48:45+08:00</updated>
    <author>
        <name>lijun</name>
    </author>
    <id>https://pure419.github.io</id>

    <entry>
        <title>CLIP</title>
        <author>
            <name>lijun</name>
        </author>
        <link href="https://pure419.github.io/clip.html"/>
        <id>https://pure419.github.io/clip.html</id>

        <updated>2024-06-03T18:48:45+08:00</updated>
            <summary>
                <![CDATA[
                    优点 迁移学习能力强（在任意一个其他数据集上效果也很好）zero shot OCR 动作检测定位 训练和分类时都不需要有提前定好的类列表 分类任务变为对比任务，只需要学习图片文本是否配对，简单了很多，不需要预测 基于对比学习的训练非常高效 背景 之前的工作不能做到随心所欲地检测，如果有新的类就无能为力，不灵活 方法 对角线正样本，一旦有正负样本，就可以用对比学习的方式进行学习， 没有分类头，如何做推理 巧妙的用自然语言处理，prompt template 预训练的时候看到的都是句子，如果突然变成一个单词，效果下降。 相似性，挑出最相似的句子 MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset 概述： 准确有效地注释大规模数据集通常成本高昂且困难，特别是对于一些需要细粒度标签的专业领域。在这种情况下，粗标签更容易获得，因为它们不需要专业知识。在这项工作中，我们提出了一种称为掩模对比学习（MaskCon）的对比学习方法来解决未充分探索的问题设置，其中我们使用粗标记数据集进行学习，以解决更精细的标记问题。更具体地说，在对比学习框架内，对于每个样本，我们的方法借助针对其他样本的粗标签和相关样本的另一个增强视图生成软标签。与自监督对比学习（其中仅样本的增强被视为硬阳性）相比，在监督对比学习中，仅具有相同粗标签的样本被视为硬阳性，**我们提出基于样本距离的软标签，这些标签被粗糙的标签。**这使我们能够利用样本间关系和粗标签。我们证明，我们的方法可以作为特殊情况获得许多现有的最先进的作品，并且它为泛化误差提供了更严格的界限。 研究背景： 准确有效地注释大规模数据集通常成本高昂且困难，粗标签更容易获得，因为它们不需要专业知识 主要贡献： 掩模对比学习（MaskCon）粗标记数据集进行学习，以解决更精细的标记问题 方法 监督学习 yi是label，pi是提取的特征 对比学习 qi是相似度，zi是样本间关系 自监督对比学习 只有自己和自己是正样本对 监督对比学习 如果label一样就是正样本对 Baseline 自监督对比损失+监督对比损失 自监督对比损失+交叉熵损失 MaskCon 自监督对比损失+掩码对比损失
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <ul>
<li><h1 id="优点">优点</h1>
<ul>
<li>迁移学习能力强（在任意一个其他数据集上效果也很好）zero shot</li>
<li>OCR 动作检测定位</li>
<li>训练和分类时都不需要有提前定好的类列表</li>
<li>分类任务变为对比任务，只需要学习图片文本是否配对，简单了很多，不需要预测</li>
<li>基于对比学习的训练非常高效</li>
</ul>
</li>
<li><h1 id="背景">背景</h1>
<ul>
<li>之前的工作不能做到随心所欲地检测，如果有新的类就无能为力，不灵活</li>
</ul>
</li>
<li><h1 id="方法">方法</h1>
<ul>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1697353811934_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
<li>对角线正样本，一旦有正负样本，就可以用对比学习的方式进行学习，</li>
<li>没有分类头，如何做推理</li>
<li>巧妙的用自然语言处理，prompt template</li>
<li>预训练的时候看到的都是句子，如果突然变成一个单词，效果下降。</li>
<li>相似性，挑出最相似的句子</li>
</ul>
</li>
<li></li>
<li><h3 id="maskcon-masked-contrastive-learning-for-coarse-labelled-dataset">MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset</h3>
</li>
<li><h4 id="概述：">概述：</h4>
<ul>
<li>准确有效地注释大规模数据集通常成本高昂且困难，特别是对于一些需要细粒度标签的专业领域。在这种情况下，粗标签更容易获得，因为它们不需要专业知识。在这项工作中，我们提出了一种称为掩模对比学习（MaskCon）的对比学习方法来解决未充分探索的问题设置，其中我们使用粗标记数据集进行学习，以解决更精细的标记问题。更具体地说，在对比学习框架内，对于每个样本，我们的方法借助针对其他样本的粗标签和相关样本的另一个增强视图生成软标签。与自监督对比学习（其中仅样本的增强被视为硬阳性）相比，在监督对比学习中，仅具有相同粗标签的样本被视为硬阳性，**我们提出基于样本距离的软标签，这些标签被粗糙的标签。**这使我们能够利用样本间关系和粗标签。我们证明，我们的方法可以作为特殊情况获得许多现有的最先进的作品，并且它为泛化误差提供了更严格的界限。</li>
</ul>
</li>
<li><h4 id="研究背景：">研究背景：</h4>
<ul>
<li>准确有效地注释大规模数据集通常成本高昂且困难，粗标签更容易获得，因为它们不需要专业知识</li>
</ul>
</li>
<li><h4 id="主要贡献：">主要贡献：</h4>
<ul>
<li>掩模对比学习（MaskCon）粗标记数据集进行学习，以解决更精细的标记问题</li>
</ul>
</li>
<li><h4 id="方法-1">方法</h4>
<ul>
<li>监督学习<ul>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701141007141_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
<li>yi是label，pi是提取的特征</li>
</ul>
</li>
<li>对比学习<ul>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701141049131_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
<li>qi是相似度，zi是样本间关系</li>
</ul>
</li>
<li>自监督对比学习<ul>
<li>只有自己和自己是正样本对</li>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701141141977_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
</ul>
</li>
<li>监督对比学习<ul>
<li>如果label一样就是正样本对</li>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701141157623_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
</ul>
</li>
<li>Baseline<ul>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701164756310_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
<li>自监督对比损失+监督对比损失</li>
<li>自监督对比损失+交叉熵损失</li>
</ul>
</li>
<li>MaskCon<ul>
<li><figure class="post__image"><img loading="lazy" src="../assets/image_1701169350830_0.png" alt="image.png"  data-is-external-image="true"></figure></li>
<li>自监督对比损失+掩码对比损失</li>
</ul>
</li>
</ul>
</li>
</ul>

            ]]>
        </content>
    </entry>
</feed>
